# .env.example

# Colima runtime: set to "docker" if you have Docker installed, or "containerd" to use the default nerdctl runtime.
# Options: "containerd" (default, uses nerdctl), "docker" (uses Docker if installed)
COLIMA_RUNTIME=containerd

# Path for persistent k8s pod storage (use home-relative path for distribution)
K8S_POD_STORAGE_PATH=~/storage_k8s_pods

# Colima VM memory size in GiB (set to a value within your system's available RAM)
# Suggestions: 8 (safe for most), 16 (for more workloads), 48 (only if you have >64GB RAM)
COLIMA_MEMORY=8

# litellm
# Azure OpenAI Resource base URL (do not share your real resource endpoint)
AZURE_API_BASE=https://[hidden].openai.azure.com/
# Azure API Key (do not use real secret in example)
AZURE_API_KEY=azurekey-example
# Azure API Version
AZURE_API_VERSION=2024-12-01-preview
# Master key for proxy (do not use real secret in example)
PROXY_MASTER_KEY=proxy-master-key-example

# mcpo
# (No environment variables defined in values.yaml; placeholder if needed)
# e.g., MCPO_API_KEY=

# milvus
ETCD_QUOTA_BACKEND_BYTES=
ETCD_HEARTBEAT_INTERVAL=
ETCD_ELECTION_TIMEOUT=

# openwebui
OPENAI_API_KEY=sk-xxx-example
OPENAI_API_BASE_URLS=http://litellm.litellm:4000
DEFAULT_MODELS=gpt-4.1
MILVUS_URI=http://milvus.milvus:19530
MILVUS_TOKEN=
MILVUS_DB=default
VECTOR_DB=milvus

# agentzero
OPENAI_API_BASE=http://litellm.litellm:4000

# OPENAI CREDS. Once LiteLLM is up, you generate those in your keys under each app. This way you can track consumption per app.
API_KEY_OPENAI_A0=sk-example
API_KEY_OPENAI_OPENWEBUI=sk-example